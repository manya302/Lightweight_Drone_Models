# -*- coding: utf-8 -*-
"""Manya Drone MobileNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CWNE2BvlQEoivBn07kPqYiZ8t56EBof6
"""

# Install aria2 for faster multi-threaded downloads
!apt install aria2 -y

# Create directory for dataset
!mkdir -p /content/coco2017

# Download TRAIN images (~14GB)
print("Downloading train2017 images...")
!aria2c -x 16 -s 16 http://images.cocodataset.org/zips/train2017.zip -d /content/coco2017 -o train2017.zip
print("âœ… train2017 download complete!")

# Download VALIDATION images (~1GB)
print("Downloading val2017 images...")
!aria2c -x 16 -s 16 http://images.cocodataset.org/zips/val2017.zip -d /content/coco2017 -o val2017.zip
print("âœ… val2017 download complete!")

# Download ANNOTATIONS (~800MB)
print("Downloading annotations...")
!aria2c -x 16 -s 16 http://images.cocodataset.org/annotations/annotations_trainval2017.zip -d /content/coco2017 -o annotations_trainval2017.zip
print("âœ… Annotations download complete!")

# Extract train images
print("Extracting train2017 images...")
!unzip -q /content/coco2017/train2017.zip -d /content/coco2017/
print("âœ… Extraction of train2017 complete!")

# Extract val images
print("Extracting val2017 images...")
!unzip -q /content/coco2017/val2017.zip -d /content/coco2017/
print("âœ… Extraction of val2017 complete!")

# Extract annotations
print("Extracting annotations...")
!unzip -q /content/coco2017/annotations_trainval2017.zip -d /content/coco2017/
print("âœ… Extraction of annotations complete!")

# Remove zip files to free space
!rm /content/coco2017/*.zip

# Verify extracted files
print("âœ… Dataset structure:")
!ls -lh /content/coco2017/
!ls -lh /content/coco2017/train2017 | head -10
!ls -lh /content/coco2017/annotations | head -10

!pip install tensorflow opencv-python pillow tqdm

import json

ANNOTATIONS_PATH = "/content/coco2017/annotations/instances_train2017.json"

with open(ANNOTATIONS_PATH, "r") as f:
    coco_data = json.load(f)

# Print keys in the dataset
print("Keys in COCO dataset:", coco_data.keys())

# Print a sample annotation
print("\nSample annotation:")
print(json.dumps(coco_data["annotations"][0], indent=4))

import os

train_path = "/content/coco2017/train2017"
val_path = "/content/coco2017/val2017"

num_train_images = len(os.listdir(train_path))
num_val_images = len(os.listdir(val_path))

print(f"Train images: {num_train_images}")
print(f"Validation images: {num_val_images}")

import os
import shutil
import json
import random

# Define paths
data_path = "/content/coco2017"
train_images_path = os.path.join(data_path, "train2017")
val_images_path = os.path.join(data_path, "val2017")
annotations_path = os.path.join(data_path, "annotations/instances_train2017.json")

# Load annotations with correct encoding
with open(annotations_path, "r", encoding="utf-8") as f:
    coco_data = json.load(f)

# Shuffle training images and move some to validation
total_train_images = len(os.listdir(train_images_path))  # Check count
new_train_count = int(total_train_images * 0.8)  # 80% for training

# Get all train images
train_images = [img['file_name'] for img in coco_data['images']]
random.shuffle(train_images)  # Shuffle for randomness

# Move extra images to validation
for img_name in train_images[new_train_count:]:
    src_path = os.path.join(train_images_path, img_name)
    dest_path = os.path.join(val_images_path, img_name)
    if os.path.exists(src_path):
        shutil.move(src_path, dest_path)

print("âœ… Train-Validation split adjusted to 80:20!")

import os
import json
from tqdm import tqdm

# Define paths
base_dir = "/content/coco2017"
datasets = ["train2017", "val2017"]  # Process both train & val

def convert_bbox(size, bbox):
    dw = 1. / size[0]
    dh = 1. / size[1]
    x = (bbox[0] + bbox[2] / 2.0) * dw
    y = (bbox[1] + bbox[3] / 2.0) * dh
    w = bbox[2] * dw
    h = bbox[3] * dh
    return x, y, w, h

for dataset in datasets:
    annotations_path = os.path.join(base_dir, "annotations", f"instances_{dataset}.json")
    yolo_labels_dir = os.path.join(base_dir, "labels", dataset)
    os.makedirs(yolo_labels_dir, exist_ok=True)

    with open(annotations_path, 'r') as f:
        coco_data = json.load(f)

    images_info = {img['id']: img for img in coco_data['images']}
    categories = {cat['id']: cat['name'] for cat in coco_data['categories']}

    for ann in tqdm(coco_data['annotations'], desc=f"Converting {dataset} Annotations"):
        img_id = ann['image_id']
        img_info = images_info.get(img_id)
        if img_info is None:
            continue

        img_w, img_h = img_info['width'], img_info['height']
        category_id = ann['category_id']
        bbox = ann['bbox']
        yolo_bbox = convert_bbox((img_w, img_h), bbox)

        label_file = os.path.join(yolo_labels_dir, f"{str(img_id).zfill(12)}.txt")
        with open(label_file, 'a') as f:
            f.write(f"{category_id} {' '.join(map(str, yolo_bbox))}\n")

    print(f"âœ… {dataset} conversion complete!")

!ls -lh /content/coco2017/labels/val2017 | head -10
!ls -lh /content/coco2017/labels/train2017 | head -10

#Import Required Libraries
import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from torchvision.models.detection import ssdlite320_mobilenet_v3_large
import os
import cv2
import numpy as np

#Define the YOLO Dataset Loader
import torch
import os
import cv2
import numpy as np

class YoloCocoDataset(torch.utils.data.Dataset):
    def __init__(self, image_dir, label_dir, transform=None):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.transform = transform
        self.image_files = sorted(os.listdir(image_dir))
        self.label_files = sorted(os.listdir(label_dir))

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        # Load image
        img_path = os.path.join(self.image_dir, self.image_files[idx])
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (320, 320))  # Resize for SSD
        img = img.astype(np.float32) / 255.0  # Normalize

        # Load YOLO label file
        label_path = os.path.join(self.label_dir, self.label_files[idx].replace('.jpg', '.txt'))
        boxes = []
        labels = []

        if os.path.exists(label_path):
            with open(label_path, "r") as f:
                for line in f.readlines():
                    parts = line.strip().split()
                    class_id = int(parts[0])  # First value is the class
                    x_center, y_center, width, height = map(float, parts[1:])  # Convert YOLO format

                    # Convert YOLO (x_center, y_center, width, height) -> (x_min, y_min, x_max, y_max)
                    x_min = (x_center - width / 2) * 320  # Scale to image size
                    y_min = (y_center - height / 2) * 320
                    x_max = (x_center + width / 2) * 320
                    y_max = (y_center + height / 2) * 320

                    boxes.append([x_min, y_min, x_max, y_max])
                    labels.append(class_id)

        # Convert to tensors
        boxes = torch.tensor(boxes, dtype=torch.float32)
        labels = torch.tensor(labels, dtype=torch.int64)

        # Apply transformations
        if self.transform:
            img = self.transform(img)

        # Return image and target dictionary
        target = {"boxes": boxes, "labels": labels}

        return img, target

from PIL import Image
import os
import torch
import time
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights

# âœ… Dataset Class
class YoloCocoDataset(Dataset):
    def __init__(self, image_dir, label_dir, transform=None):
        self.image_dir = image_dir
        self.label_dir = label_dir
        self.transform = transform
        self.images = sorted(os.listdir(image_dir))
        self.labels = sorted(os.listdir(label_dir))

        # Match images with labels
        image_names = {img.rsplit(".", 1)[0] for img in os.listdir(image_dir)}
        label_names = {lbl.rsplit(".", 1)[0] for lbl in os.listdir(label_dir)}
        common_names = sorted(image_names & label_names)

        self.images = [f"{name}.jpg" for name in common_names]
        self.labels = [f"{name}.txt" for name in common_names]
        print(f"âœ… Filtered dataset: {len(self.images)} matching image-label pairs.")

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.images[idx])
        image = Image.open(img_path).convert("RGB")

        label_filename = self.images[idx].replace(".jpg", ".txt")
        label_path = os.path.join(self.label_dir, label_filename)

        if not os.path.exists(label_path):
            print(f"Warning: Label missing for {self.images[idx]} -> Skipping sample")
            return None

        boxes = []
        with open(label_path, "r") as f:
            for line in f.readlines():
                parts = list(map(float, line.split()[1:]))
                x_min, y_min, x_max, y_max = parts
                if x_max > x_min and y_max > y_min:
                    boxes.append(parts)

        if len(boxes) == 0:
            return None

        target = {
            "boxes": torch.tensor(boxes, dtype=torch.float32),
            "labels": torch.ones((len(boxes),), dtype=torch.int64)
        }

        if self.transform:
            image = self.transform(image)

        return image, target

# âœ… Custom Collate Function
def collate_fn(batch):
    batch = [b for b in batch if b is not None]
    if len(batch) < 2:
        return None, None

    images, targets = zip(*batch)
    images = torch.stack(images)
    return images, targets

# âœ… Dataset Paths
train_dir = "/content/coco2017/train2017"
train_labels = "/content/coco2017/labels/train2017"
val_dir = "/content/coco2017/val2017"
val_labels = "/content/coco2017/labels/val2017"

# âœ… Define Transform
transform = transforms.Compose([
    transforms.Resize((320, 320)),
    transforms.ToTensor(),
])

# âœ… Create Datasets and DataLoaders
train_dataset = YoloCocoDataset(train_dir, train_labels, transform)
val_dataset = YoloCocoDataset(val_dir, val_labels, transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)

# âœ… Load Pretrained MobileNet Model
weights = SSDLite320_MobileNet_V3_Large_Weights.COCO_V1
model = ssdlite320_mobilenet_v3_large(weights=weights)
model.train()

# âœ… Set Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# âœ… Define Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix, ConfusionMatrixDisplay
from torchvision.ops import box_iou

# âœ… Initialize Lists for Metrics
losses = []
precisions = []
recalls = []
mAPs = []

# âœ… Metric Calculation Function
def calculate_metrics(model, val_loader, device, iou_threshold=0.5):
    model.eval()
    all_scores = []
    all_labels = []

    with torch.no_grad():
        for images, targets in val_loader:
            if images is None:
                continue

            images = [img.to(device) for img in images]
            outputs = model(images)

            for i, output in enumerate(outputs):
                pred_boxes = output['boxes'].cpu()
                pred_scores = output['scores'].cpu()
                true_boxes = targets[i]['boxes'].cpu()

                if len(pred_boxes) == 0:
                    continue

                ious = box_iou(pred_boxes, true_boxes)
                max_ious, _ = ious.max(dim=1)

                tp = (max_ious >= iou_threshold).float()
                fp = (max_ious < iou_threshold).float()

                all_scores.extend(pred_scores.tolist())
                all_labels.extend(tp.tolist())

    if len(all_scores) == 0:
        return [], [], 0, np.zeros((2, 2))

    precision, recall, _ = precision_recall_curve(all_labels, all_scores)
    avg_precision = average_precision_score(all_labels, all_scores)
    cm = confusion_matrix(all_labels, np.round(all_scores))

    return precision, recall, avg_precision, cm

# âœ… Training Loop
num_epochs = 10
for epoch in range(num_epochs):
    start_time = time.time()
    total_loss = 0
    model.train()

    print(f"ðŸš€ Epoch {epoch + 1}/{num_epochs} started...")

    for images, targets in train_loader:
        if images is None:
            continue

        optimizer.zero_grad()
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        loss = sum(loss for loss in loss_dict.values())

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    epoch_time = time.time() - start_time
    avg_loss = total_loss / len(train_loader)
    losses.append(avg_loss)

    precision, recall, avg_precision, cm = calculate_metrics(model, val_loader, device)
    precisions.append(precision)
    recalls.append(recall)
    mAPs.append(avg_precision)

    print(f"âœ… Epoch {epoch + 1} completed. Loss = {avg_loss:.4f}, mAP = {avg_precision:.4f}, Time = {epoch_time:.2f} sec")

# âœ… Training Summary
print("\nðŸ“Š Training Summary:")
for epoch, loss, mAP in zip(range(1, num_epochs + 1), losses, mAPs):
    print(f"Epoch {epoch}: Loss = {loss:.4f}, mAP = {mAP:.4f}")

# âœ… Plot Loss Curve
plt.figure()
plt.plot(range(1, num_epochs + 1), losses, marker='o', label='Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss vs. Epochs')
plt.legend()
plt.show()

# âœ… Plot Precision-Recall Curve
plt.figure()
plt.plot(recalls[-1], precisions[-1], marker='.', label=f'mAP: {mAPs[-1]:.4f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()

# âœ… Plot Confusion Matrix
plt.figure()
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.title('Confusion Matrix')
plt.show()

print("âœ… Training complete!")